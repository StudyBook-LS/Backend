{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Study categorization with scispacy.\n",
    "\n",
    "Import the things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import multiprocessing\n",
    "import pprint\n",
    "import re\n",
    "import spacy\n",
    "import scispacy\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from stopwords import STOPWORDS\n",
    "from tqdm import tqdm_notebook\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from scispacy.abbreviation import AbbreviationDetector\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up scispacy with additional stopwords, abbreviation detection and entity detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")\n",
    "\n",
    "nlp.Defaults.stop_words |= STOPWORDS\n",
    "\n",
    "abbreviation_pipe = AbbreviationDetector(nlp)\n",
    "nlp.add_pipe(abbreviation_pipe)\n",
    "\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True,\n",
    "                         k=10,\n",
    "                         max_entities_per_mention=3)\n",
    "nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure hashability for faster lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = dict()\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    stopwords[word] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define import functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def replace_csv_missing(row):\n",
    "    \"\"\"Do basic text cleaning from csv rows.\"\"\"\n",
    "    text = (row[0] + ' ' + \n",
    "            row[1] + ' ' + \n",
    "            row[8] + ' ' + \n",
    "            ', '.join(row[2].split()) + ', ' + \n",
    "            ', '.join(row[3].split()) + ', ' + \n",
    "            ', '.join(row[6].split()) + ', ' + \n",
    "            ', '.join(row[7].split())\n",
    "           )\n",
    "    new = re.sub('missing', '', text)\n",
    "    new = re.sub('-', ' ', new)\n",
    "    new = re.sub('\\(\\S*\\)', ' ', new)\n",
    "    new = re.sub(' \\s+', ' ', new)\n",
    "    new = re.sub(',,', ',', new)\n",
    "    return u\"{}\".format(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_files(file):\n",
    "    all_rows = []\n",
    "    with open(file, 'r') as csvinfile:\n",
    "        csv_reader = csv.reader(csvinfile, delimiter=',')\n",
    "#         for row in csv_reader:\n",
    "#             all_rows.append(row)\n",
    "        # Size limit for development.\n",
    "        for i, row in enumerate(csv_reader):\n",
    "            if i > 1000:\n",
    "                break\n",
    "            all_rows.append(row)\n",
    "    pool = Pool(processes=multiprocessing.cpu_count())\n",
    "    with pool as p:\n",
    "        transformed_rows = p.map(replace_csv_missing, all_rows)\n",
    "    return transformed_rows\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = import_csv_files('../all_trials_text.csv')[1:]  # Discard header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll need a place to store our index for fast lookup. [This](https://aws.amazon.com/blogs/aws/amazon-dynamodb-internet-scale-data-storage-the-nosql-way/) looks promising.\n",
    "\n",
    "below is a json like idea that we should be able to upload to a db easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_entry(entry: str, index: int, dictionary: defaultdict) -> None:\n",
    "    \"\"\"Make an index entry with look-up terms as keys, tuples of document indexes and\n",
    "    relevancy score as values.\n",
    "    \n",
    "    :var entry: text from which terms are to be extracted.\n",
    "    :var index: index of text\n",
    "    :var dictionary: dictionary to update with new terms and indexes.\n",
    "    \"\"\"\n",
    "    doc = nlp(entry)\n",
    "    count = Counter()\n",
    "    for text in doc.ents:\n",
    "        if text.lemma_.lower() not in stopwords:\n",
    "            count[text.lemma_.lower()] += 1\n",
    "    total = sum(count.values())\n",
    "    for term, relevancy in count.items():\n",
    "        dictionary[term].update({index: round(relevancy / total, 4)})\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(corpus: list) -> defaultdict:\n",
    "    \"\"\"Create a dict index of look-up terms.\n",
    "    \n",
    "    Look-up terms as keys, tuples of document indexes and\n",
    "    relevancy scores as values.\n",
    "    \n",
    "    :var corpus: list of list of strings from which to extract look-up terms.\n",
    "    \n",
    "    :returns dictionary: dict of dicts \"look-up term\": {index: relevancy score},\n",
    "    \"\"\"\n",
    "    dictionary = defaultdict(dict)\n",
    "    for i, text in tqdm_notebook(enumerate(corpus)):\n",
    "        make_entry(text, i, dictionary)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e8f9bf9274bb99b6327a2717f0bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = create_index(search_terms)\n",
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for display purposes. Just the entries ending in 'a'.\n",
    "filtered_index = dict(filter(lambda item: item[0].endswith('a'), index.items()))\n",
    "print(len(filtered_index))\n",
    "pprint.pprint(filtered_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unit4 (Python 3.7)",
   "language": "python",
   "name": "unit4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
