{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Perceptron on XOR Gates](#Q2)\n",
    "3. [Multilayer Perceptron](#Q3)\n",
    "4. [Keras MMP](#Q4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** \n",
    "    The base unit for a network. Applies weights and biases to inputs, then an activatin function before passing info \n",
    "    to the next level.\n",
    "- **Input Layer:** \n",
    "    The layer that takes in the data from the dataset.\n",
    "- **Hidden Layer:** \n",
    "    Layers between the input and output layers.\n",
    "- **Output Layer:**\n",
    "    The final layer. This is where the prediction is made.\n",
    "- **Activation:**\n",
    "    A function applied to the result of multiplying input by weight and adding bias. Determines if node 'fires' and at what degree.\n",
    "- **Backpropagation:**\n",
    "    A function for updating weights and biases at the beginning of the network based off of the last iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perceptron on XOR Gates <a id=\"Q2\"></a>\n",
    "\n",
    "The XOr, or “exclusive or”, problem is a classic problem in ANN research. It is the problem of using a neural network to predict the outputs of XOr logic gates given two binary inputs. An XOr function should return a true value if the two inputs are not equal and a false value if they are equal. Create a perceptron class that can model the behavior of an AND gate. You can use the following table as your training data:\n",
    "\n",
    "|x1\t|x2 | y |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "| 1 | 0 | 1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    \"\"\"Perceptron estimator with early stopping.\n",
    "    \n",
    "    :param learning_rate: float Estimator learning rate. Default == 0.01\n",
    "    :param epochs: int Number of epochs to run Perceptron. Default = 1000\n",
    "    :param early_stopping: int Number of epochs without imoprovement at which to stop estimator. Default = 10\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, epochs=100, early_stopping=10):\n",
    "        self.lr = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        \n",
    "    def predict(self,row):\n",
    "        \"\"\"Apply weights and add bias to inputs.\n",
    "        \n",
    "        Return 1 if output is greater or equal zero, else zero for each element in input row.\n",
    "        \"\"\"\n",
    "        \n",
    "        return (np.dot(row, self.weight[1:]) + self.weight[0]) >= 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        \n",
    "        Initialize with random bias and weights.\n",
    "        Update weights and bias with each row based on previous iteration's error.\n",
    "        Store number of errors for each epoch.\n",
    "        Stop if no errors in number of `early_stopping` epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.weight = np.array([np.random.random() for _ in range(X.shape[1] + 1)])\n",
    "    \n",
    "        self.errors_ = []\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            error = 0\n",
    "            for row, label in zip(X, y):\n",
    "                \n",
    "                # Check our current prediction against the actual label to get the error.\n",
    "                # Multiply the result by the learning rate.\n",
    "                adjustment = self.lr * (label - self.predict(row))\n",
    "                \n",
    "                # Adjust our weigts and bias accordingly.\n",
    "                self.weight[1:] += adjustment * row\n",
    "                self.weight[0] += adjustment\n",
    "                \n",
    "                # Add up our errors for each epoch.\n",
    "                error += adjustment != 0.0\n",
    "                \n",
    "            # Make a list of number of errors per epoch.\n",
    "            self.errors_.append(error)\n",
    "\n",
    "            # If we've been correct each time for a number of rounds, stop already.\n",
    "            if sum(self.errors_[-self.early_stopping:]) == 0:\n",
    "#                 print('Stopped Early')\n",
    "                break\n",
    "                \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePerceptron(object):\n",
    "    \"\"\"Combines output of two Perceptrons as input to a final Perceptron.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.perc = Perceptron()\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit two Perceptrons to the data, zip outputs together to use as input\n",
    "        for self.perc.\n",
    "        \"\"\"\n",
    "        self.one = Perceptron().fit(X, y)\n",
    "        self.two = Perceptron().fit(X, y)\n",
    "        first = self.one.predict(X)\n",
    "        second = self.two.predict(X)\n",
    "        inputs = np.array([np.array([one, two]) for one, two in zip(first, second)])\n",
    "        self.perc.fit(inputs, y)\n",
    "        \n",
    "    def _predict(self, X):\n",
    "        \"\"\"Use predictions from self.one and self.two to predict yhat from X.\"\"\"\n",
    "        first = self.one.predict(X)\n",
    "        second = self.two.predict(X)\n",
    "\n",
    "        try:\n",
    "            inputs = np.array([np.array([one, two]) for one, two in zip(first, second)])\n",
    "        except TypeError as e:\n",
    "            inputs = np.array([first, second])\n",
    "        return self.perc.predict(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False, False, False])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xor = DoublePerceptron()\n",
    "\n",
    "Xor = np.array([np.array([0, 0]),\n",
    "                np.array([1, 0]),\n",
    "                np.array([0, 1]),\n",
    "                np.array([1, 1])])\n",
    "\n",
    "yor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "xor.fit(Xor, yor)\n",
    "\n",
    "xor._predict(np.array([np.array([1, 0]),\n",
    "                      np.array([1, 1]),\n",
    "                      np.array([0, 1]), \n",
    "                      np.array([0, 0])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights.\n",
    "Your network must have one hidden layer.\n",
    "You do not have to update weights via gradient descent. You can use something like the derivative of the sigmoid function to update weights.\n",
    "Train your model on the Heart Disease dataset from UCI:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>282</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>1</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "62    52    1   3       118   186    0        0      190      0      0.0   \n",
       "150   66    1   0       160   228    0        0      138      0      2.3   \n",
       "239   35    1   0       126   282    0        0      156      1      0.0   \n",
       "65    35    0   0       138   183    0        1      182      0      1.4   \n",
       "268   54    1   0       122   286    0        0      116      1      3.2   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "62       1   0     1       1  \n",
       "150      2   0     1       1  \n",
       "239      2   0     3       0  \n",
       "65       2   0     2       1  \n",
       "268      1   2     2       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='target').values\n",
    "y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((303, 13), (303,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    \n",
    "    def __init__(self, epochs=10000, learning_rate=0.01, n_input=13, n_hidden=64, n_out=1):\n",
    "        \n",
    "        # Initialize hyperparameter variables.\n",
    "        self.epochs = epochs\n",
    "        self.lr = learning_rate\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_out = n_out\n",
    "    \n",
    "        # Initialize weights and biases.\n",
    "        self.hidden_weight = np.random.random(size=(self.n_input + 1, self.n_hidden))\n",
    "        self.output_weight = np.random.random(size=(self.n_hidden + 1, self.n_out))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_prime(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.errors = []\n",
    "        for i in range(self.epochs):\n",
    "            out = self.predict(X)\n",
    "            self.backpass(X, y, out)\n",
    "        print(f'Training error at {i} epoch: {self.errors[-1]}')\n",
    "\n",
    "    def backpass(self, X, y, out):\n",
    "        y = y.reshape((y.shape[0], 1))\n",
    "        error = y - out\n",
    "        \n",
    "        self.errors.append(np.sum(error**2))\n",
    "        # Caluculate adjustment from hidden -> output.\n",
    "        delta_output = self.sigmoid_prime(out) * error\n",
    "        \n",
    "        # Calculate error from input -> hidden.\n",
    "        output_error = delta_output.dot(self.output_weight[1:].T)\n",
    "        delta_hidden = output_error * self.sigmoid_prime(out)\n",
    "        \n",
    "        #Adjust hidden -> output weghts.\n",
    "\n",
    "        self.output_weight[1:] += self.activated_hidden.T.dot(delta_output) * self.lr\n",
    "        self.output_weight[0] = np.sum(delta_output)\n",
    "\n",
    "        self.hidden_weight[1:] += X.T.dot(delta_hidden) * self.lr\n",
    "        self.hidden_weight[0] = np.sum(delta_hidden)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        inputs = np.dot(X, self.hidden_weight[1:]) + self.hidden_weight[0]\n",
    "        self.activated_hidden = self.sigmoid(inputs)\n",
    "        output = np.dot(self.activated_hidden, self.output_weight[1:]) + self.output_weight[0]\n",
    "        final = self.sigmoid(output)\n",
    "        return final\n",
    "        \n",
    "    def plot_error(self):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.title('Training Error')\n",
    "        plt.plot(self.errors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "better = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error at 9999 epoch: 31.522235220614412\n"
     ]
    }
   ],
   "source": [
    "better.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86302619]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_one = [37, 1, 2, 130, 250, 0, 1, 187, 0, 3.5, 0, 0, 2]\n",
    "p_one = scaler.transform([p_one])\n",
    "better.predict(p_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_two = [56, 1, 1, 120, 236, 0, 1, 178, 0, 0.8, 2, 0, 2]\n",
    "p_two = scaler.transform([p_two])\n",
    "better.predict(p_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08120308]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_three = [63, 0, 0, 108, 269, 0, 1, 169, 1, 1.8, 1, 2, 2]\n",
    "p_three = scaler.transform([p_three])\n",
    "better.predict(p_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Keras MMP <a id=\"Q4\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 5 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_creator(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='relu', input_dim=inputs))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=model_creator, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = X, y\n",
    "\n",
    "inputs = X.shape[1]\n",
    "epochs = 20\n",
    "batch_size = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.6299 - acc: 0.6980\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 505us/step - loss: 0.5296 - acc: 0.7871\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 505us/step - loss: 0.4601 - acc: 0.8218\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 461us/step - loss: 0.4135 - acc: 0.8267\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 522us/step - loss: 0.3843 - acc: 0.8465\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 509us/step - loss: 0.3616 - acc: 0.8515\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 511us/step - loss: 0.3452 - acc: 0.8614\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 474us/step - loss: 0.3307 - acc: 0.8614\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 542us/step - loss: 0.3199 - acc: 0.8713\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 542us/step - loss: 0.3124 - acc: 0.8812\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 650us/step - loss: 0.2999 - acc: 0.8960\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 711us/step - loss: 0.2909 - acc: 0.9010\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 552us/step - loss: 0.2835 - acc: 0.8861\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 542us/step - loss: 0.2752 - acc: 0.9010\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 473us/step - loss: 0.2681 - acc: 0.8861\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 611us/step - loss: 0.2598 - acc: 0.9158\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.2543 - acc: 0.9010\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 1ms/step - loss: 0.2454 - acc: 0.9109\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 437us/step - loss: 0.2398 - acc: 0.9208\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 430us/step - loss: 0.2319 - acc: 0.9109\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 0.6198 - acc: 0.6733\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 624us/step - loss: 0.5191 - acc: 0.7772\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 498us/step - loss: 0.4470 - acc: 0.8366\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 399us/step - loss: 0.3933 - acc: 0.8366\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 396us/step - loss: 0.3605 - acc: 0.8515\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 377us/step - loss: 0.3360 - acc: 0.8564\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 383us/step - loss: 0.3179 - acc: 0.8515\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 397us/step - loss: 0.3065 - acc: 0.8663\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 389us/step - loss: 0.2959 - acc: 0.8614\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 417us/step - loss: 0.2849 - acc: 0.8614\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 393us/step - loss: 0.2779 - acc: 0.8663\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 391us/step - loss: 0.2748 - acc: 0.8762\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 390us/step - loss: 0.2618 - acc: 0.8861\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 494us/step - loss: 0.2560 - acc: 0.8762\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 404us/step - loss: 0.2502 - acc: 0.8812\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 383us/step - loss: 0.2411 - acc: 0.8812\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 393us/step - loss: 0.2376 - acc: 0.8812\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 387us/step - loss: 0.2297 - acc: 0.8960\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 379us/step - loss: 0.2264 - acc: 0.8861\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 417us/step - loss: 0.2185 - acc: 0.8960\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.6604 - acc: 0.6436\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 446us/step - loss: 0.5514 - acc: 0.8218\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 404us/step - loss: 0.4717 - acc: 0.8465\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 391us/step - loss: 0.4113 - acc: 0.8762\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 423us/step - loss: 0.3634 - acc: 0.8812\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 413us/step - loss: 0.3301 - acc: 0.8911\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 387us/step - loss: 0.3053 - acc: 0.8911\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 424us/step - loss: 0.2893 - acc: 0.8911\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 384us/step - loss: 0.2782 - acc: 0.9059\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 417us/step - loss: 0.2640 - acc: 0.9158\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 394us/step - loss: 0.2537 - acc: 0.9158\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 411us/step - loss: 0.2466 - acc: 0.9158\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 408us/step - loss: 0.2396 - acc: 0.9257\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 398us/step - loss: 0.2306 - acc: 0.9307\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 402us/step - loss: 0.2236 - acc: 0.9356\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 427us/step - loss: 0.2159 - acc: 0.9307\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 385us/step - loss: 0.2114 - acc: 0.9356\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 406us/step - loss: 0.2038 - acc: 0.9356\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 426us/step - loss: 0.1988 - acc: 0.9356\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 422us/step - loss: 0.1908 - acc: 0.9406\n",
      "101/101 [==============================] - 0s 3ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 10ms/step - loss: 0.6649 - acc: 0.6436\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.6295 - acc: 0.7030\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.5962 - acc: 0.7178\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 0.5676 - acc: 0.7574\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.5402 - acc: 0.7723\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.5194 - acc: 0.8020\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.5037 - acc: 0.8119\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 102us/step - loss: 0.4867 - acc: 0.8119\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.4701 - acc: 0.8267\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.4576 - acc: 0.8218\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 104us/step - loss: 0.4433 - acc: 0.8267\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 104us/step - loss: 0.4328 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 111us/step - loss: 0.4213 - acc: 0.8317\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 0.4128 - acc: 0.8317\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 103us/step - loss: 0.4035 - acc: 0.8267\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.3978 - acc: 0.8267\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 0.3913 - acc: 0.8317\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 147us/step - loss: 0.3857 - acc: 0.8366\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.3807 - acc: 0.8317\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.3765 - acc: 0.8366\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.7217 - acc: 0.4653\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.6681 - acc: 0.5545\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 0.6243 - acc: 0.6980\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 0.5882 - acc: 0.7475\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.5579 - acc: 0.7921\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 0.5292 - acc: 0.7970\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 129us/step - loss: 0.5050 - acc: 0.7921\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 130us/step - loss: 0.4819 - acc: 0.8020\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.4624 - acc: 0.8069\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.4457 - acc: 0.8119\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.4303 - acc: 0.8069\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.4164 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.4026 - acc: 0.8218\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.3904 - acc: 0.8218\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 0.3799 - acc: 0.8366\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.3706 - acc: 0.8366\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 0.3620 - acc: 0.8317\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 119us/step - loss: 0.3550 - acc: 0.8267\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 113us/step - loss: 0.3483 - acc: 0.8218\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 113us/step - loss: 0.3430 - acc: 0.8317\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.6800 - acc: 0.5891\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 122us/step - loss: 0.6397 - acc: 0.6436\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.6096 - acc: 0.7030\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 0.5830 - acc: 0.7327\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.5594 - acc: 0.7673\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.5388 - acc: 0.7822\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.5183 - acc: 0.7921\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 0.4996 - acc: 0.7970\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 139us/step - loss: 0.4816 - acc: 0.7921\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 143us/step - loss: 0.4638 - acc: 0.7921\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 130us/step - loss: 0.4459 - acc: 0.7921\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 110us/step - loss: 0.4309 - acc: 0.8020\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 0.4166 - acc: 0.8119\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.4027 - acc: 0.8267\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.3916 - acc: 0.8366\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 107us/step - loss: 0.3820 - acc: 0.8366\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 0.3750 - acc: 0.8416\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 115us/step - loss: 0.3668 - acc: 0.8366\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 0.3594 - acc: 0.8366\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 108us/step - loss: 0.3534 - acc: 0.8416\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.7194 - acc: 0.4059\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.6959 - acc: 0.4851\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.6763 - acc: 0.5594\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 0.6593 - acc: 0.5941\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.6441 - acc: 0.6089\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 89us/step - loss: 0.6299 - acc: 0.6980\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 91us/step - loss: 0.6172 - acc: 0.7178\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.6050 - acc: 0.7376\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.5934 - acc: 0.7624\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.5837 - acc: 0.7822\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.5748 - acc: 0.8020\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 83us/step - loss: 0.5656 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 0.5555 - acc: 0.8168\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.5464 - acc: 0.8168\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.5377 - acc: 0.8267\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 85us/step - loss: 0.5287 - acc: 0.8119\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 183us/step - loss: 0.5196 - acc: 0.8069\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 160us/step - loss: 0.5091 - acc: 0.8119\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 0.4995 - acc: 0.8069\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 121us/step - loss: 0.4901 - acc: 0.8069\n",
      "101/101 [==============================] - 0s 5ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 11ms/step - loss: 0.7318 - acc: 0.4802\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.7016 - acc: 0.5198\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 73us/step - loss: 0.6765 - acc: 0.5594\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 72us/step - loss: 0.6542 - acc: 0.6238\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 79us/step - loss: 0.6364 - acc: 0.6485\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.6228 - acc: 0.6535\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.6100 - acc: 0.6683\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.5983 - acc: 0.6881\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 85us/step - loss: 0.5862 - acc: 0.7129\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.5742 - acc: 0.7327\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 0.5624 - acc: 0.7376\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.5515 - acc: 0.7525\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 0.5397 - acc: 0.7624\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 0.5266 - acc: 0.8218\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.5142 - acc: 0.8267\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 81us/step - loss: 0.5018 - acc: 0.8366\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.4901 - acc: 0.8416\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 77us/step - loss: 0.4790 - acc: 0.8416\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 0.4678 - acc: 0.8515\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 88us/step - loss: 0.4583 - acc: 0.8515\n",
      "101/101 [==============================] - 0s 4ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 2s 12ms/step - loss: 0.6834 - acc: 0.5990\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 74us/step - loss: 0.6582 - acc: 0.6040\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.6364 - acc: 0.6287\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.6171 - acc: 0.6683\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.6000 - acc: 0.6931\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 93us/step - loss: 0.5845 - acc: 0.7030\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 106us/step - loss: 0.5704 - acc: 0.7277\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.5569 - acc: 0.7376\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.5438 - acc: 0.7574\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 78us/step - loss: 0.5318 - acc: 0.7624\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 82us/step - loss: 0.5210 - acc: 0.7723\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.5107 - acc: 0.7772\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.5005 - acc: 0.7970\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 76us/step - loss: 0.4917 - acc: 0.7970\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 109us/step - loss: 0.4831 - acc: 0.8020\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 145us/step - loss: 0.4738 - acc: 0.8020\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 123us/step - loss: 0.4667 - acc: 0.8119\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 114us/step - loss: 0.4600 - acc: 0.8267\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 100us/step - loss: 0.4541 - acc: 0.8366\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - ETA: 0s - loss: 0.4724 - acc: 0.830 - 0s 106us/step - loss: 0.4476 - acc: 0.8366\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 13ms/step - loss: 0.6340 - acc: 0.6485\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6226 - acc: 0.6733\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.6117 - acc: 0.6782\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 105us/step - loss: 0.6021 - acc: 0.6881\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.5925 - acc: 0.6980\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5830 - acc: 0.7129\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.5733 - acc: 0.7327\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.5640 - acc: 0.7574\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.5551 - acc: 0.7673\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.5463 - acc: 0.7673\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.5377 - acc: 0.7772\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5294 - acc: 0.7772\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.5213 - acc: 0.7772\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.5135 - acc: 0.7921\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.5058 - acc: 0.8020\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.4983 - acc: 0.8020\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.4909 - acc: 0.8069\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 62us/step - loss: 0.4837 - acc: 0.8119\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.4767 - acc: 0.8168\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 99us/step - loss: 0.4698 - acc: 0.8218\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 0.7775 - acc: 0.2624\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.7619 - acc: 0.3168\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 60us/step - loss: 0.7470 - acc: 0.3515\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.7339 - acc: 0.3911\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.7209 - acc: 0.4208\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 75us/step - loss: 0.7079 - acc: 0.4653\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6953 - acc: 0.5149\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 83us/step - loss: 0.6830 - acc: 0.5743\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6711 - acc: 0.6188\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 92us/step - loss: 0.6596 - acc: 0.6485\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.6483 - acc: 0.6634\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.6373 - acc: 0.6782\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 38us/step - loss: 0.6265 - acc: 0.6782\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.6160 - acc: 0.7376\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 52us/step - loss: 0.6057 - acc: 0.7525\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.5956 - acc: 0.7574\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5856 - acc: 0.7624\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.5759 - acc: 0.7772\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 84us/step - loss: 0.5664 - acc: 0.7822\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.5570 - acc: 0.7673\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 16ms/step - loss: 0.7157 - acc: 0.5693\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.7012 - acc: 0.5693\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.6871 - acc: 0.5842\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6735 - acc: 0.5891\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6611 - acc: 0.6139\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6488 - acc: 0.6188\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6367 - acc: 0.6287\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6249 - acc: 0.6337\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6133 - acc: 0.6733\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 0.6021 - acc: 0.6782\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 0.5912 - acc: 0.6782\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 0.5807 - acc: 0.7228\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5704 - acc: 0.7376\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.5604 - acc: 0.7624\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5508 - acc: 0.7624\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 0.5414 - acc: 0.7772\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.5323 - acc: 0.7822\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.5236 - acc: 0.7871\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5151 - acc: 0.7921\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5068 - acc: 0.7921\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 12ms/step - loss: 0.7574 - acc: 0.4257\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 38us/step - loss: 0.7421 - acc: 0.4307\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.7275 - acc: 0.4653\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.7149 - acc: 0.4901\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.7024 - acc: 0.5099\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6902 - acc: 0.5347\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6782 - acc: 0.5842\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6670 - acc: 0.6238\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6562 - acc: 0.6386\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6459 - acc: 0.6832\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 94us/step - loss: 0.6361 - acc: 0.6980\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6266 - acc: 0.7228\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6175 - acc: 0.7376\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6088 - acc: 0.7475\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.6005 - acc: 0.7574\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.5923 - acc: 0.7673\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5844 - acc: 0.7673\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5768 - acc: 0.7723\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.5692 - acc: 0.7723\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5617 - acc: 0.7772\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 13ms/step - loss: 0.7584 - acc: 0.4158\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.7431 - acc: 0.4455\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.7260 - acc: 0.4653\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.7084 - acc: 0.5000\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6907 - acc: 0.5396\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6734 - acc: 0.5792\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6565 - acc: 0.6089\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.6401 - acc: 0.6535\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6242 - acc: 0.6931\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.6088 - acc: 0.7228\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 96us/step - loss: 0.5938 - acc: 0.7624\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5795 - acc: 0.7822\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5657 - acc: 0.8020\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5524 - acc: 0.8119\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.5396 - acc: 0.8168\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5273 - acc: 0.8267\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 38us/step - loss: 0.5155 - acc: 0.8267\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5042 - acc: 0.8267\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.4934 - acc: 0.8317\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.4830 - acc: 0.8317\n",
      "101/101 [==============================] - 1s 6ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 0.6534 - acc: 0.6931\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.6449 - acc: 0.7030\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.6354 - acc: 0.7129\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 0.6255 - acc: 0.7327\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.6151 - acc: 0.7475\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.6049 - acc: 0.7426\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.5950 - acc: 0.7624\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.5855 - acc: 0.7574\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5763 - acc: 0.7624\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.5672 - acc: 0.7574\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.5582 - acc: 0.7475\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 0.5495 - acc: 0.7574\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5409 - acc: 0.7574\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5326 - acc: 0.7723\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.5244 - acc: 0.7723\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.5164 - acc: 0.7723\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.5086 - acc: 0.7723\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.5009 - acc: 0.7772\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 30us/step - loss: 0.4935 - acc: 0.7871\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 29us/step - loss: 0.4862 - acc: 0.7921\n",
      "101/101 [==============================] - 1s 7ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 0.6585 - acc: 0.6436\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.6503 - acc: 0.6436\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6410 - acc: 0.6634\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 117us/step - loss: 0.6313 - acc: 0.6782\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 166us/step - loss: 0.6217 - acc: 0.6980\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 0.6122 - acc: 0.7129\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.6025 - acc: 0.7228\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5935 - acc: 0.7426\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.5846 - acc: 0.7525\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.5759 - acc: 0.7624\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.5674 - acc: 0.7624\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5592 - acc: 0.7723\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5512 - acc: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5435 - acc: 0.7822\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 0.5359 - acc: 0.7871\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5284 - acc: 0.7970\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.5212 - acc: 0.7970\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 86us/step - loss: 0.5141 - acc: 0.7970\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 80us/step - loss: 0.5072 - acc: 0.7921\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.5004 - acc: 0.7871\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 0.6740 - acc: 0.5842\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 32us/step - loss: 0.6653 - acc: 0.6040\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.6539 - acc: 0.6337\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6428 - acc: 0.6733\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6327 - acc: 0.6931\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6222 - acc: 0.6980\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 48us/step - loss: 0.6123 - acc: 0.7030\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6026 - acc: 0.7228\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.5930 - acc: 0.7475\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5836 - acc: 0.7624\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5745 - acc: 0.7772\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5655 - acc: 0.7871\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.5565 - acc: 0.8069\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 63us/step - loss: 0.5477 - acc: 0.8218\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5389 - acc: 0.8317\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.5303 - acc: 0.8218\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.5219 - acc: 0.8218\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.5134 - acc: 0.8366\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.5051 - acc: 0.8317\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.4968 - acc: 0.8366\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 14ms/step - loss: 0.7240 - acc: 0.4158\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.7146 - acc: 0.4307\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.7043 - acc: 0.4752\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6929 - acc: 0.5099\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6826 - acc: 0.5792\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6726 - acc: 0.6089\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.6628 - acc: 0.6535\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6533 - acc: 0.7030\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.6440 - acc: 0.7228\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6349 - acc: 0.7376\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6259 - acc: 0.7475\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6172 - acc: 0.7376\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 90us/step - loss: 0.6086 - acc: 0.7475\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 55us/step - loss: 0.6001 - acc: 0.7772\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.5916 - acc: 0.7673\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5833 - acc: 0.7772\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.5751 - acc: 0.7871\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5668 - acc: 0.7871\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 35us/step - loss: 0.5587 - acc: 0.7871\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5505 - acc: 0.7921\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 0.6550 - acc: 0.6683\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.6458 - acc: 0.6683\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6355 - acc: 0.6782\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.6250 - acc: 0.6782\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6138 - acc: 0.7079\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6034 - acc: 0.7277\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.5933 - acc: 0.7574\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5834 - acc: 0.7772\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5738 - acc: 0.7871\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 49us/step - loss: 0.5644 - acc: 0.7970\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.5552 - acc: 0.8119\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5462 - acc: 0.8168\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5375 - acc: 0.8267\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 120us/step - loss: 0.5290 - acc: 0.8317\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.5207 - acc: 0.8218\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 59us/step - loss: 0.5127 - acc: 0.8168\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 50us/step - loss: 0.5048 - acc: 0.8218\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.4971 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 56us/step - loss: 0.4897 - acc: 0.8218\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 51us/step - loss: 0.4824 - acc: 0.8218\n",
      "101/101 [==============================] - 1s 12ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 15ms/step - loss: 0.7456 - acc: 0.4158\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.7321 - acc: 0.4406\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.7146 - acc: 0.4505\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6993 - acc: 0.4851\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6841 - acc: 0.5347\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6692 - acc: 0.5644\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6547 - acc: 0.6337\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6406 - acc: 0.6634\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.6270 - acc: 0.7030\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.6137 - acc: 0.7228\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.6010 - acc: 0.7376\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 36us/step - loss: 0.5887 - acc: 0.7673\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5768 - acc: 0.7723\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 33us/step - loss: 0.5654 - acc: 0.7822\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.5544 - acc: 0.7921\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 34us/step - loss: 0.5438 - acc: 0.8020\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 37us/step - loss: 0.5337 - acc: 0.8020\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 31us/step - loss: 0.5238 - acc: 0.8168\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 87us/step - loss: 0.5143 - acc: 0.8218\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 39us/step - loss: 0.5052 - acc: 0.8267\n",
      "101/101 [==============================] - 1s 8ms/step\n",
      "Epoch 1/20\n",
      "202/202 [==============================] - 3s 17ms/step - loss: 0.6988 - acc: 0.4703\n",
      "Epoch 2/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.6890 - acc: 0.4802\n",
      "Epoch 3/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6779 - acc: 0.5545\n",
      "Epoch 4/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.6667 - acc: 0.5891\n",
      "Epoch 5/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.6555 - acc: 0.5990\n",
      "Epoch 6/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6446 - acc: 0.6287\n",
      "Epoch 7/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.6339 - acc: 0.6584\n",
      "Epoch 8/20\n",
      "202/202 [==============================] - 0s 41us/step - loss: 0.6236 - acc: 0.6980\n",
      "Epoch 9/20\n",
      "202/202 [==============================] - 0s 47us/step - loss: 0.6136 - acc: 0.7475\n",
      "Epoch 10/20\n",
      "202/202 [==============================] - 0s 40us/step - loss: 0.6039 - acc: 0.7673\n",
      "Epoch 11/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.5945 - acc: 0.7673\n",
      "Epoch 12/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5853 - acc: 0.7673\n",
      "Epoch 13/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5763 - acc: 0.7822\n",
      "Epoch 14/20\n",
      "202/202 [==============================] - 0s 43us/step - loss: 0.5675 - acc: 0.7822\n",
      "Epoch 15/20\n",
      "202/202 [==============================] - 0s 84us/step - loss: 0.5590 - acc: 0.7822\n",
      "Epoch 16/20\n",
      "202/202 [==============================] - 0s 44us/step - loss: 0.5506 - acc: 0.7822\n",
      "Epoch 17/20\n",
      "202/202 [==============================] - 0s 54us/step - loss: 0.5424 - acc: 0.7871\n",
      "Epoch 18/20\n",
      "202/202 [==============================] - 0s 46us/step - loss: 0.5343 - acc: 0.7871\n",
      "Epoch 19/20\n",
      "202/202 [==============================] - 0s 42us/step - loss: 0.5264 - acc: 0.7871\n",
      "Epoch 20/20\n",
      "202/202 [==============================] - 0s 45us/step - loss: 0.5187 - acc: 0.7871\n",
      "101/101 [==============================] - 1s 9ms/step\n",
      "Epoch 1/20\n",
      "303/303 [==============================] - 4s 12ms/step - loss: 0.6775 - acc: 0.5479\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 252us/step - loss: 0.6234 - acc: 0.7129\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 255us/step - loss: 0.5808 - acc: 0.7855\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 183us/step - loss: 0.5453 - acc: 0.7855\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 177us/step - loss: 0.5126 - acc: 0.8119\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 205us/step - loss: 0.4857 - acc: 0.8284\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 196us/step - loss: 0.4647 - acc: 0.8251\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 182us/step - loss: 0.4466 - acc: 0.8251\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 245us/step - loss: 0.4281 - acc: 0.8284\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 198us/step - loss: 0.4116 - acc: 0.8350\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 183us/step - loss: 0.3975 - acc: 0.8383\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 182us/step - loss: 0.3860 - acc: 0.8317\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 201us/step - loss: 0.3776 - acc: 0.8218\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 186us/step - loss: 0.3695 - acc: 0.8251\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 178us/step - loss: 0.3618 - acc: 0.8284\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 193us/step - loss: 0.3562 - acc: 0.8383\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 191us/step - loss: 0.3494 - acc: 0.8416\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 134us/step - loss: 0.3452 - acc: 0.8383\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 149us/step - loss: 0.3403 - acc: 0.8515\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 164us/step - loss: 0.3334 - acc: 0.8515\n",
      "Best: 0.834983498349835 using {'batch_size': 50, 'epochs': 20}\n",
      "Mean: 0.7986798652327887, Stdev: 0.02034460041457361 with : {'batch_size': 10, 'epochs': 20}\n",
      "Mean: 0.834983498349835, Stdev: 0.030606008508093193 with : {'batch_size': 50, 'epochs': 20}\n",
      "Mean: 0.801980190151202, Stdev: 0.016168257800612366 with : {'batch_size': 100, 'epochs': 20}\n",
      "Mean: 0.7557755708694458, Stdev: 0.05848200778543977 with : {'batch_size': 250, 'epochs': 20}\n",
      "Mean: 0.7788778940836588, Stdev: 0.0336568844486241 with : {'batch_size': 500, 'epochs': 20}\n",
      "Mean: 0.7656765778859457, Stdev: 0.02034459460670113 with : {'batch_size': 1000, 'epochs': 20}\n",
      "Mean: 0.7722772359848022, Stdev: 0.024252366463151962 with : {'batch_size': 2500, 'epochs': 20}\n"
     ]
    }
   ],
   "source": [
    "params = {'batch_size': [10, 50, 100, 250, 500, 1000, 2500],\n",
    "          'epochs': [20]}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params)\n",
    "grid_result = grid.fit(X, y)\n",
    "print(f'Best: {grid_result.best_score_} using {grid_result.best_params_}')\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Mean: {mean}, Stdev: {stdev} with : {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "303/303 [==============================] - 3s 10ms/step - loss: 0.5203 - acc: 0.7723\n",
      "Epoch 2/20\n",
      "303/303 [==============================] - 0s 150us/step - loss: 0.4052 - acc: 0.8350\n",
      "Epoch 3/20\n",
      "303/303 [==============================] - 0s 146us/step - loss: 0.3702 - acc: 0.8515\n",
      "Epoch 4/20\n",
      "303/303 [==============================] - 0s 127us/step - loss: 0.3557 - acc: 0.8647\n",
      "Epoch 5/20\n",
      "303/303 [==============================] - 0s 125us/step - loss: 0.3433 - acc: 0.8680\n",
      "Epoch 6/20\n",
      "303/303 [==============================] - 0s 149us/step - loss: 0.3344 - acc: 0.8713\n",
      "Epoch 7/20\n",
      "303/303 [==============================] - 0s 140us/step - loss: 0.3269 - acc: 0.8812\n",
      "Epoch 8/20\n",
      "303/303 [==============================] - 0s 157us/step - loss: 0.3225 - acc: 0.8878\n",
      "Epoch 9/20\n",
      "303/303 [==============================] - 0s 128us/step - loss: 0.3186 - acc: 0.8878\n",
      "Epoch 10/20\n",
      "303/303 [==============================] - 0s 143us/step - loss: 0.3157 - acc: 0.8944\n",
      "Epoch 11/20\n",
      "303/303 [==============================] - 0s 124us/step - loss: 0.3102 - acc: 0.8812\n",
      "Epoch 12/20\n",
      "303/303 [==============================] - 0s 144us/step - loss: 0.3072 - acc: 0.8944\n",
      "Epoch 13/20\n",
      "303/303 [==============================] - 0s 144us/step - loss: 0.3035 - acc: 0.8878\n",
      "Epoch 14/20\n",
      "303/303 [==============================] - 0s 158us/step - loss: 0.3005 - acc: 0.8878\n",
      "Epoch 15/20\n",
      "303/303 [==============================] - 0s 137us/step - loss: 0.2968 - acc: 0.8911\n",
      "Epoch 16/20\n",
      "303/303 [==============================] - 0s 151us/step - loss: 0.2937 - acc: 0.8911\n",
      "Epoch 17/20\n",
      "303/303 [==============================] - 0s 150us/step - loss: 0.2902 - acc: 0.8911\n",
      "Epoch 18/20\n",
      "303/303 [==============================] - 0s 144us/step - loss: 0.2882 - acc: 0.8977\n",
      "Epoch 19/20\n",
      "303/303 [==============================] - 0s 150us/step - loss: 0.2859 - acc: 0.8911\n",
      "Epoch 20/20\n",
      "303/303 [==============================] - 0s 136us/step - loss: 0.2842 - acc: 0.8878\n",
      "Best: 0.8184818442505185 using {'epochs': 20, 'optimizer': 'adagrad'}\n",
      "Mean: 0.7887788727731988, Stdev: 0.03645332596892404 with : {'epochs': 20, 'optimizer': 'adam'}\n",
      "Mean: 0.8184818442505185, Stdev: 0.02333685607392056 with : {'epochs': 20, 'optimizer': 'adagrad'}\n",
      "Mean: 0.7491749206391891, Stdev: 0.05382674417391531 with : {'epochs': 20, 'optimizer': 'sgd'}\n"
     ]
    }
   ],
   "source": [
    "params = {'optimizer': ['adam', 'adagrad', 'sgd'],\n",
    "          'epochs': [20]}\n",
    "grid1 = GridSearchCV(estimator=grid_result.best_estimator_, param_grid= params,\n",
    "                   n_jobs=-1)\n",
    "grid_result1 = grid1.fit(X, y, verbose=1)\n",
    "\n",
    "print(f'Best: {grid_result1.best_score_} using {grid_result1.best_params_}')\n",
    "means = grid_result1.cv_results_['mean_test_score']\n",
    "stds = grid_result1.cv_results_['std_test_score']\n",
    "params = grid_result1.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f'Mean: {mean}, Stdev: {stdev} with : {param}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.7 (unit4wk2)",
   "language": "python",
   "name": "unit4wk2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
